{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    \"\"\"\n",
    "    list the txt files in dataset\n",
    "    \n",
    "    parameter: file path, str\n",
    "    \n",
    "    return: list\n",
    "    \"\"\"\n",
    "#     path = './dataset 1/train/spam'\n",
    "    all_file = os.listdir(file_path) \n",
    "   \n",
    "    for i in range(len(all_file)):\n",
    "        all_file[i] = file_path + all_file[i]\n",
    "    return all_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dict_from_file(file_path):\n",
    "    '''\n",
    "    Build directory\n",
    "    parameter: file_path\n",
    "    return: directory of each file \n",
    "    '''\n",
    "    result = {\"?? ??????????????\": 1}\n",
    "    file = open(file_path, 'r', errors='ignore')\n",
    "    \n",
    "    for line in file:\n",
    "        word_list = line.rstrip('\\n').lower().split()\n",
    "        for word in word_list:\n",
    "            if word not in result:\n",
    "                result[word] = 1\n",
    "            else:\n",
    "                result[word] += 1\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dict_list):\n",
    "    '''\n",
    "    parameter: \n",
    "        dict_list: a list of dictionary\n",
    "\n",
    "    establish a list of words and weight\n",
    "\n",
    "    result: list of words and weight\n",
    "    '''\n",
    "    W = {\"?? ??????????????\": 0}\n",
    "    for sub_dict in dict_list:\n",
    "        for word in sub_dict[0]:\n",
    "            W[word] = 0      \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, w):\n",
    "    \"\"\"\n",
    "    Param:\n",
    "        x: dictionary\n",
    "        w: weight\n",
    "    \"\"\"\n",
    "    z = 0\n",
    "    \n",
    "    for word in x:\n",
    "        if word in w:\n",
    "            z += x[word] * w[word]\n",
    "    if z > 10:\n",
    "        z = 10\n",
    "    if z < -10:\n",
    "        z = -10\n",
    "    \n",
    "    return 1 / (1 + np.exp(z))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w):\n",
    "    '''\n",
    "    input: \n",
    "    \n",
    "    output:\n",
    "    \n",
    "    '''\n",
    "    predicted_label = sigmoid(x, w)\n",
    "    if predicted_label > 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dict_list, weight):\n",
    "    '''\n",
    "    parameter: \n",
    "        dict_list: list of dictionaries from each dataset in testset\n",
    "        weight: train   \n",
    "    '''\n",
    "    correct = 0\n",
    "    for element in dict_list:\n",
    "        x, label = element[0], element[1]\n",
    "        predict_label= predict(x, weight)\n",
    "        if predict_label == label:\n",
    "            correct += 1\n",
    "            \n",
    "    accuracy = correct / len(dict_list)\n",
    "    \n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dict_list, weight, iteration, eta, regular):\n",
    "    \"\"\"\n",
    "    Param:\n",
    "        dict_list: (x, label), list of dictionaries from each dataset\n",
    "        weight:\n",
    "        spam_ham: label of current dict_list\n",
    "    Return:\n",
    "    \"\"\"\n",
    "    for t in range(iteration):\n",
    "        dw = {word: 0 for word in weight}\n",
    "#         lw = 0\n",
    "        for element in dict_list:\n",
    "            x, label = element[0], element[1]\n",
    "            p = sigmoid(x, weight)\n",
    "#             lw += label * np.log(p) + (1-label) * np.log(1-p)\n",
    "            for word in weight:\n",
    "                if word in x:\n",
    "                    dw[word] += x[word] * (label - (1-p))\n",
    "        for i in dw:\n",
    "            weight[i] += eta * dw[i] - regular * weight[i] * eta\n",
    "        \n",
    "#         print(f'\\r{t/iteration}%', end='')\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def develop_data(data, a): \n",
    "    ham_train_path = f'{data}/{a}/ham/'\n",
    "    spam_train_path = f'{data}/{a}/spam/'\n",
    "    \n",
    "#     print(ham_train_path)\n",
    "#     print(spam_train_path)\n",
    "    \n",
    "    ham_file_train = load_file(ham_train_path)\n",
    "    spam_file_train = load_file(spam_train_path)\n",
    "    \n",
    "    print(f'{data}/{a}/ham/: {len(ham_file_train)}')\n",
    "    print(f'{data}/{a}/spam/: {len(spam_file_train)}')\n",
    "\n",
    "    ham_gen_dict = []\n",
    "    for file in ham_file_train:\n",
    "        ham_gen_dict.append(gen_dict_from_file(file))\n",
    "        \n",
    "    spam_gen_dict = []\n",
    "    for file in spam_file_train:\n",
    "        spam_gen_dict.append(gen_dict_from_file(file))\n",
    "    \n",
    "    total_dict_train = [(x, 1) for x in ham_gen_dict] + [(x, 0) for x in spam_gen_dict]\n",
    "    \n",
    "    return total_dict_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1\n",
      "./datasets/dataset 1/train/ham/: 340\n",
      "./datasets/dataset 1/train/spam/: 123\n",
      "acc: 0.8920863309352518, lambda: 0.01\n"
     ]
    }
   ],
   "source": [
    "dataset = ['dataset 1', 'dataset 2', 'dataset 3']\n",
    "\n",
    "# train_data = develop_data('./datasets/dataset 1/', 'train')\n",
    "for data in dataset:\n",
    "    print(data)\n",
    "    \n",
    "    train_data = develop_data('./datasets/'+data, 'train')\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    weight = initialize(train_data)\n",
    "    \n",
    "#     print(f'weight size: {len(weight)}')\n",
    "    \n",
    "    split_point = math.floor(len(train_data) * 0.7) \n",
    "    \n",
    "#     panalize_param = [0.1, 1, 3, 5, 10, 20, 30, 50, 100]\n",
    "    panalize_param = [0.01, 0, 10]\n",
    "    \n",
    "    weight_candidate = [initialize(train_data) for _ in range(len(panalize_param))]\n",
    "    \n",
    "# #     print(len(weight_candidate))\n",
    "# #     print(weight_candidate[0])\n",
    "\n",
    "    best_acc = 0\n",
    "    best_panalize_param = 0\n",
    "    iterations = 400\n",
    "    eta = 1e-4\n",
    "    \n",
    "    for i in range(len(panalize_param)):\n",
    "        \n",
    "        weight_candidate[i] = train(train_data[:split_point],\n",
    "                                    weight_candidate[i], \n",
    "                                    iterations, \n",
    "                                    eta, \n",
    "                                    panalize_param[i])\n",
    "    \n",
    "        # Validate\n",
    "        accuracy = test(train_data[split_point:], weight_candidate[i])\n",
    "        \n",
    "        print(f'acc: {accuracy}, lambda: {panalize_param[i]}')\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_panalize_param = panalize_param[i]\n",
    "    \n",
    "    print(f'best lambda: {best_panalize_param}')\n",
    "    \n",
    "    weight_final = train(train_data, initialize(train_data), iterations, eta, best_panalize_param)\n",
    "    \n",
    "    test_data = develop_data('./datasets/'+data, 'test')\n",
    "    accuracy_final = test(test_data, weight_final)\n",
    "    \n",
    "    print(\n",
    "        f'-----------------------\\n'\n",
    "        f'Acc on test: {accuracy_final}\\n'\n",
    "        f'\\n#######################\\n'\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
