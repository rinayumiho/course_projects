{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNlR2hlSHzL3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1570300284838,
     "user": {
      "displayName": "Yun Tang",
      "photoUrl": "",
      "userId": "16761261370194733199"
     },
     "user_tz": 240
    },
    "id": "0GcxTSNAHzL6",
    "outputId": "a142e36f-bbf9-4e9d-db3c-5124b382d330"
   },
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    \"\"\"\n",
    "    list the txt files in dataset\n",
    "    \n",
    "    parameter: file path, str\n",
    "    \n",
    "    return: list\n",
    "    \"\"\"\n",
    "#     path = './dataset 1/train/spam'\n",
    "    all_file = os.listdir(file_path) \n",
    "   \n",
    "    for i in range(len(all_file)):\n",
    "        all_file[i] = file_path + all_file[i]\n",
    "    return all_file\n",
    "\n",
    "# a = load_file('./datasets/dataset 1/train/ham')\n",
    "# print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zjAEoA2aHzL_"
   },
   "outputs": [],
   "source": [
    "def gen_dict_from_file(file_path):\n",
    "    '''\n",
    "    Build directory\n",
    "    parameter: file_path\n",
    "    return: directory of each file \n",
    "    '''\n",
    "    result = {\"????????????????\": 1}\n",
    "    file = open(file_path, 'r', errors='ignore')\n",
    "    \n",
    "    for line in file:\n",
    "        word_list = line.rstrip('\\n').lower().split()\n",
    "        for word in word_list:\n",
    "            if word not in result:\n",
    "                result[word] = 1\n",
    "            else:\n",
    "                result[word] += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZXX6eq_brFu"
   },
   "outputs": [],
   "source": [
    "def initialize(dict_list):\n",
    "    '''\n",
    "    parameter: \n",
    "        dict_list: a list of dictionary\n",
    "\n",
    "    establish a list of words and weight\n",
    "\n",
    "    result: list of words and weight\n",
    "    '''\n",
    "    W = {\"????????????????\": 0}\n",
    "    for sub_dict in dict_list:\n",
    "        for word in sub_dict[0]:\n",
    "                W[word] = 0      \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEylXbPRHzL9"
   },
   "outputs": [],
   "source": [
    "def predict(x, w):\n",
    "    \"\"\"\n",
    "    Use sign function to predict label\n",
    "    \"\"\"\n",
    "    z = 0\n",
    "    \n",
    "    for word in x:\n",
    "        if word in w:\n",
    "            z += x[word] * w[word]\n",
    "            \n",
    "    if z > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dict_list, weight):\n",
    "    '''\n",
    "    parameter: \n",
    "        dict_list: list of dictionaries from each dataset in testset\n",
    "        weight: train   \n",
    "    '''\n",
    "    correct = 0\n",
    "    for element in dict_list:\n",
    "        x, label = element[0], element[1]\n",
    "        predict_label= predict(x, weight)\n",
    "        if predict_label == label:\n",
    "            correct += 1\n",
    "            \n",
    "    accuracy = correct / len(dict_list)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dict_list, weight, iteration, eta):\n",
    "    \"\"\"\n",
    "    Param:\n",
    "        dict_list: (x, label), list of dictionaries from each dataset\n",
    "        weight:\n",
    "        spam_ham: label of current dict_list\n",
    "    Return:\n",
    "    \"\"\"\n",
    "    for t in range(iteration):\n",
    "        for element in dict_list:\n",
    "            x, label = element[0], element[1]\n",
    "            p = predict(x, weight)\n",
    "            for word in x:\n",
    "                weight[word] += eta * (label - p) * x[word]\n",
    "                \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def develop_data(data, a): \n",
    "    ham_train_path = f'{data}/{a}/ham/'\n",
    "    spam_train_path = f'{data}/{a}/spam/'\n",
    "    \n",
    "    ham_file_train = load_file(ham_train_path)\n",
    "    spam_file_train = load_file(spam_train_path)\n",
    "    \n",
    "    print(f'{data}/{a}/ham/: {len(ham_file_train)}')\n",
    "    print(f'{data}/{a}/spam/: {len(spam_file_train)}')\n",
    "\n",
    "    ham_gen_dict = []\n",
    "    for file in ham_file_train:\n",
    "        ham_gen_dict.append(gen_dict_from_file(file))\n",
    "        \n",
    "    spam_gen_dict = []\n",
    "    for file in spam_file_train:\n",
    "        spam_gen_dict.append(gen_dict_from_file(file))\n",
    "    \n",
    "    total_dict_train = [(x, 1) for x in ham_gen_dict] + [(x, -1) for x in spam_gen_dict]\n",
    "    \n",
    "    return total_dict_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1\n",
      "./datasets/dataset 1/train/ham/: 340\n",
      "./datasets/dataset 1/train/spam/: 123\n",
      "acc: 0.7553956834532374, lambda: 1\n",
      "acc: 0.9136690647482014, lambda: 10\n",
      "acc: 0.9136690647482014, lambda: 30\n",
      "acc: 0.9136690647482014, lambda: 50\n",
      "best iter: 10\n",
      "./datasets/dataset 1/test/ham/: 348\n",
      "./datasets/dataset 1/test/spam/: 130\n",
      "-----------------------\n",
      "Acc on test: 0.8807531380753139\n",
      "\n",
      "#######################\n",
      "\n",
      "dataset 2\n",
      "./datasets/dataset 2/train/ham/: 319\n",
      "./datasets/dataset 2/train/spam/: 131\n",
      "acc: 0.837037037037037, lambda: 1\n",
      "acc: 0.8962962962962963, lambda: 10\n",
      "acc: 0.9185185185185185, lambda: 30\n",
      "acc: 0.9185185185185185, lambda: 50\n",
      "best iter: 30\n",
      "./datasets/dataset 2/test/ham/: 307\n",
      "./datasets/dataset 2/test/spam/: 149\n",
      "-----------------------\n",
      "Acc on test: 0.9035087719298246\n",
      "\n",
      "#######################\n",
      "\n",
      "dataset 3\n",
      "./datasets/dataset 3/train/ham/: 133\n",
      "./datasets/dataset 3/train/spam/: 402\n",
      "acc: 0.7018633540372671, lambda: 1\n",
      "acc: 0.906832298136646, lambda: 10\n",
      "acc: 0.8944099378881988, lambda: 30\n",
      "acc: 0.8944099378881988, lambda: 50\n",
      "best iter: 10\n",
      "./datasets/dataset 3/test/ham/: 152\n",
      "./datasets/dataset 3/test/spam/: 391\n",
      "-----------------------\n",
      "Acc on test: 0.9502762430939227\n",
      "\n",
      "#######################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = ['dataset 1', 'dataset 2', 'dataset 3']\n",
    "\n",
    "for data in dataset:\n",
    "    print(data)\n",
    "    \n",
    "    train_data = develop_data('./datasets/'+data, 'train')\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "#     weight = initialize(train_data)\n",
    "    \n",
    "#     print(f'weight size: {len(weight)}')\n",
    "    \n",
    "    split_point = math.floor(len(train_data) * 0.7) \n",
    "    \n",
    "#     panalize_param = [0.1, 1, 3, 5, 10, 20, 30, 50, 100]\n",
    "    iterations = [1, 10, 30, 50]\n",
    "    \n",
    "    weight_candidate = [initialize(train_data) for _ in range(len(iterations))]\n",
    "\n",
    "    best_acc = 0\n",
    "    best_iterations = 0\n",
    "    eta = 1e-4\n",
    "    \n",
    "    for i in range(len(iterations)):\n",
    "        \n",
    "        weight_candidate[i] = train(train_data[:split_point], weight_candidate[i], iterations[i], eta)\n",
    "    \n",
    "        # Validate\n",
    "        accuracy = test(train_data[split_point:], weight_candidate[i])\n",
    "        \n",
    "        print(f'acc: {accuracy}, # iter: {iterations[i]}')\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_iterations = iterations[i]\n",
    "    \n",
    "    print(f'best iter: {best_iterations}')\n",
    "    \n",
    "    weight_final = train(train_data, initialize(train_data), best_iterations, eta)\n",
    "#     weight_final = train(train_data, initialize(train_data), 30, 1e-4)\n",
    "    \n",
    "    test_data = develop_data('./datasets/'+data, 'test')\n",
    "    \n",
    "    accuracy_final = test(test_data, weight_final)\n",
    "    \n",
    "    print(\n",
    "        f'-----------------------\\n'\n",
    "        f'Acc on test: {accuracy_final}\\n'\n",
    "        f'\\n#######################\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    for _ in range(10000000):\n",
    "        pass\n",
    "    print('\\r',i, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.12\n"
     ]
    }
   ],
   "source": [
    "a = 10.123142345\n",
    "print(f'{a:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(type(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "10.0\n",
      "100.0\n",
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "for i in [-3, -2, -1, 1, 2, 3]:\n",
    "    print(float('1e'+str(i)))\n",
    "    print(f'{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw2 perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
